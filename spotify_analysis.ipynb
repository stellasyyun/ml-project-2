{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spotify Song Popularity Analysis - Practice Assignment\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates the process of analyzing Spotify song features to predict song popularity using machine learning. This is a practice assignment to showcase data science workflow and documentation.\n",
    "\n",
    "## Steps We'll Follow:\n",
    "1. **Setup and Data Loading**\n",
    "   - Import required libraries\n",
    "   - Load and examine the dataset\n",
    "\n",
    "2. **Data Preprocessing**\n",
    "   - Clean the data\n",
    "   - Engineer new features\n",
    "   - Transform categorical variables\n",
    "\n",
    "3. **Model Development**\n",
    "   - Split data into training and testing sets\n",
    "   - Scale features\n",
    "   - Train and optimize model\n",
    "\n",
    "4. **Evaluation and Visualization**\n",
    "   - Assess model performance\n",
    "   - Create visualizations\n",
    "   - Document insights\n",
    "\n",
    "## Note:\n",
    "This is a practice exercise. Each step will be documented with explanations of what we're doing and why. The focus is on understanding the machine learning workflow and proper documentation practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn')\n",
    "sns.set_palette('viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Initial Data Loading\n",
    "\n",
    "First, we'll import all necessary libraries for our analysis. Each library serves a specific purpose:\n",
    "- pandas: for data manipulation and analysis\n",
    "- numpy: for numerical operations\n",
    "- scikit-learn: for machine learning tools\n",
    "- matplotlib/seaborn: for visualization\n",
    "\n",
    "We're also setting visual styles for consistent, professional-looking plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('../Untitled/Resources/spotify_songs.csv')\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nColumns:\", df.columns.tolist())\n",
    "print(\"\\nSample of the data:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Preprocessing and Feature Engineering\n",
    "\n",
    "In this section, we'll:\n",
    "1. Create popularity categories (High/Medium/Low) instead of using raw scores\n",
    "   - High: â‰¥67 (top 25%)\n",
    "   - Medium: 34-66 (middle 50%)\n",
    "   - Low: <34 (bottom 25%)\n",
    "\n",
    "2. Engineer new features:\n",
    "   - energy_danceability: combining energy and danceability\n",
    "   - loudness_scaled: normalized loudness values\n",
    "   - tempo_scaled: normalized tempo values\n",
    "\n",
    "3. Handle categorical variables:\n",
    "   - One-hot encode genre information\n",
    "\n",
    "We'll visualize the distribution of our target variable to understand class balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create popularity categories\n",
    "def categorize_popularity(x):\n",
    "    if x >= 67:  # Top 25%\n",
    "        return 'High'\n",
    "    elif x >= 34:  # Middle 50%\n",
    "        return 'Medium'\n",
    "    else:  # Bottom 25%\n",
    "        return 'Low'\n",
    "\n",
    "df['popularity_category'] = df['track_popularity'].apply(categorize_popularity)\n",
    "\n",
    "# Feature engineering\n",
    "df['energy_danceability'] = df['energy'] * df['danceability']\n",
    "df['loudness_scaled'] = (df['loudness'] - df['loudness'].min()) / (df['loudness'].max() - df['loudness'].min())\n",
    "df['tempo_scaled'] = df['tempo'] / df['tempo'].max()\n",
    "\n",
    "# One-hot encode genre\n",
    "genre_dummies = pd.get_dummies(df['playlist_genre'], prefix='genre')\n",
    "df = pd.concat([df, genre_dummies], axis=1)\n",
    "\n",
    "# Display the distribution of popularity categories\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=df, x='popularity_category', order=['Low', 'Medium', 'High'])\n",
    "plt.title('Distribution of Song Popularity Categories')\n",
    "plt.xlabel('Popularity Category')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Feature Selection and Data Preparation\n",
    "\n",
    "Now we'll prepare our data for modeling:\n",
    "1. Select relevant features:\n",
    "   - Audio features (danceability, energy, etc.)\n",
    "   - Engineered features\n",
    "   - Genre information\n",
    "\n",
    "2. Split data into training and testing sets:\n",
    "   - 80% training, 20% testing\n",
    "   - Use stratification to maintain class distribution\n",
    "\n",
    "3. Scale features:\n",
    "   - Use StandardScaler to normalize feature ranges\n",
    "   - Fit scaler on training data only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Select features for prediction\n",
    "audio_features = [\n",
    "    'danceability', 'energy', 'key', 'loudness_scaled', 'speechiness',\n",
    "    'acousticness', 'instrumentalness', 'liveness', 'valence',\n",
    "    'tempo_scaled', 'energy_danceability'\n",
    "]\n",
    "\n",
    "genre_columns = [col for col in df.columns if col.startswith('genre_')]\n",
    "features = audio_features + genre_columns\n",
    "\n",
    "# Prepare X (features) and y (target)\n",
    "X = df[features]\n",
    "y = df['popularity_category']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Training set shape:\", X_train_scaled.shape)\n",
    "print(\"Testing set shape:\", X_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Model Training and Optimization\n",
    "\n",
    "We'll use GridSearchCV with RandomForestClassifier to find the best model:\n",
    "1. Define parameter grid:\n",
    "   - n_estimators: number of trees\n",
    "   - max_depth: tree depth\n",
    "   - min_samples_split/leaf: controls tree structure\n",
    "\n",
    "2. Train model with cross-validation:\n",
    "   - 5-fold cross-validation\n",
    "   - Parallel processing enabled\n",
    "   - Track best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define parameter grid for GridSearch\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "# Initialize and train model with GridSearch\n",
    "rf_model = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Best parameters:\", rf_model.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Model Evaluation\n",
    "\n",
    "Let's evaluate our model's performance:\n",
    "1. Generate predictions on test set\n",
    "2. Calculate key metrics:\n",
    "   - Overall accuracy\n",
    "   - Per-class precision, recall, F1-score\n",
    "   - Confusion matrix\n",
    "\n",
    "This will help us understand how well our model performs across different popularity categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Make predictions with best model\n",
    "best_model = rf_model.best_estimator_\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy Score: {accuracy:.2f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Results Visualization\n",
    "\n",
    "We'll create visualizations to understand our results:\n",
    "1. Confusion Matrix:\n",
    "   - Shows prediction accuracy for each category\n",
    "   - Highlights where model makes mistakes\n",
    "\n",
    "2. Feature Importance:\n",
    "   - Top 15 most influential features\n",
    "   - Helps understand what drives song popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['High', 'Low', 'Medium'],\n",
    "            yticklabels=['High', 'Low', 'Medium'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# Feature importance plot\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': features,\n",
    "    'importance': best_model.feature_importances_\n",
    "})\n",
    "feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='importance', y='feature', data=feature_importance.head(15))\n",
    "plt.title('Top 15 Features for Predicting Song Popularity')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Save and Document Results\n",
    "\n",
    "Finally, we'll save our results for future reference:\n",
    "1. Create dedicated output directory\n",
    "2. Save various outputs:\n",
    "   - Feature importance rankings\n",
    "   - Model predictions\n",
    "   - Performance metrics\n",
    "\n",
    "This creates a permanent record of our analysis and findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create output directory\n",
    "output_dir = 'analysis_output'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Save feature importance\n",
    "feature_importance.to_csv(os.path.join(output_dir, 'feature_importance.csv'), index=False)\n",
    "\n",
    "# Save predictions\n",
    "predictions_df = pd.DataFrame({\n",
    "    'Actual': y_test,\n",
    "    'Predicted': y_pred\n",
    "})\n",
    "predictions_df.to_csv(os.path.join(output_dir, 'predictions.csv'), index=False)\n",
    "\n",
    "# Save model performance metrics\n",
    "with open(os.path.join(output_dir, 'model_performance.txt'), 'w') as f:\n",
    "    f.write(f\"Best Parameters: {rf_model.best_params_}\\n\")\n",
    "    f.write(f\"Accuracy Score: {accuracy:.2f}\\n\")\n",
    "    f.write(\"\\nClassification Report:\\n\")\n",
    "    f.write(class_report)\n",
    "\n",
    "print(\"Results saved to\", output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
